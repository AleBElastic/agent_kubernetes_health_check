# Find Deployments with issues:

FROM metrics-k8sclusterreceiver.otel-default, metrics-kubeletstatsreceiver.otel-default
| WHERE @timestamp > NOW() - 1 hour
| EVAL has_issue = CASE(
    k8s.deployment.available < k8s.deployment.desired, 1,
    k8s.container.restarts > 3, 1,
    k8s.container.ready == 0, 1,
    k8s.container.memory_limit_utilization > 0.9, 1,
    k8s.container.cpu_limit_utilization > 0.9, 1,
    0
  )
| WHERE has_issue == 1
| KEEP @timestamp, resource.attributes.k8s.namespace.name, resource.attributes.k8s.deployment.name, 
       resource.attributes.k8s.pod.name, resource.attributes.k8s.container.name, 
       k8s.container.ready, k8s.container.restarts, k8s.deployment.available, 
       k8s.deployment.desired, k8s.container.memory_limit_utilization, 
       k8s.container.cpu_limit_utilization, has_issue
| SORT @timestamp DESC
| LIMIT 100


# Any container that is not ready or has restarted or has deployment issues ? 

FROM metrics-k8sclusterreceiver.otel-default, metrics-kubeletstatsreceiver.otel-default
| WHERE @timestamp > NOW() - 1 hour
| EVAL has_issue = CASE(
    k8s.deployment.available < k8s.deployment.desired, 1,
    k8s.container.restarts > 3, 1,
    k8s.container.ready == 0, 1,
    k8s.container.memory_limit_utilization > 0.9, 1,
    k8s.container.cpu_limit_utilization > 0.9, 1,
    0
  )
| WHERE has_issue == 1
| KEEP @timestamp, resource.attributes.k8s.namespace.name, resource.attributes.k8s.deployment.name, 
       resource.attributes.k8s.pod.name, resource.attributes.k8s.container.name, 
       k8s.container.ready, k8s.container.restarts, k8s.deployment.available, 
       k8s.deployment.desired, k8s.container.memory_limit_utilization, 
       k8s.container.cpu_limit_utilization, has_issue
| STATS 
    latest_timestamp = MAX(@timestamp),
    latest_ready = TOP(k8s.container.ready, 1, "desc"),
    latest_restarts = TOP(k8s.container.restarts, 1, "desc"),
    latest_available = TOP(k8s.deployment.available, 1, "desc"),
    latest_desired = TOP(k8s.deployment.desired, 1, "desc"),
    latest_memory_util = TOP(k8s.container.memory_limit_utilization, 1, "desc"),
    latest_cpu_util = TOP(k8s.container.cpu_limit_utilization, 1, "desc")
  BY 
    resource.attributes.k8s.namespace.name,
    resource.attributes.k8s.deployment.name,
    resource.attributes.k8s.pod.name,
    resource.attributes.k8s.container.name
| SORT latest_restarts ASC
| LIMIT 100


----

# Find Logs for failing application that should be a parameter

FROM logs-*
| WHERE @timestamp > NOW() - 1 hour
| WHERE log.level == "ERROR" OR severity_text == "ERROR" OR message LIKE "*error*"
| KEEP @timestamp, message, resource.attributes.k8s.pod.name, resource.attributes.k8s.namespace.name, resource.attributes.k8s.container.name, log.level, severity_text
| STATS latest_timestamp = MAX(@timestamp), latest_message = TOP(message, 1, "desc") BY resource.attributes.k8s.container.name
| SORT latest_timestamp DESC
| LIMIT 100


The key difference in this query is how the TOP function is used:

TOP(message, 1, "desc") - This returns the top 1 message value sorted in descending order
The syntax requires the sort direction to be a string literal ("desc" or "asc") in quotes
Unlike my previous incorrect suggestion, you don't specify the field to sort by within the TOP function - it uses the field you're selecting (message)
This approach gives you the message with the highest lexicographical value for each container. If you specifically want the message that corresponds to the exact timestamp in latest_timestamp, you would need a more complex approach, potentially using subqueries or ROW_NUMBER which are not currently available in ES|QL.

For most use cases, either the MAX(message) or TOP(message, 1, "desc") approach should provide useful results, though they select messages based on different criteria.

understand you want to include the message column in your query results after the STATS command. The challenge here is that when you use STATS with a BY clause, it groups your data, and you can only access the columns you ve grouped by or aggregated

-----

## Find Logs with error

FROM logs-*
| WHERE @timestamp > NOW() - 1 hour
| WHERE log.level == "ERROR" OR severity_text == "ERROR" OR message LIKE "*error*"
| KEEP @timestamp, message, resource.attributes.k8s.pod.name, resource.attributes.k8s.namespace.name, resource.attributes.k8s.container.name, log.level, severity_text
| SORT @timestamp DESC
| LIMIT 100

----

# Find Resources utilisation and provide recommendations on usage


---> Final <-----

FROM metrics-*
| WHERE @timestamp >= NOW() - 7 days
| WHERE k8s.namespace.name == "chaos-demo" AND (k8s.container.memory_request_utilization IS NOT NULL OR k8s.container.cpu_request_utilization IS NOT NULL)
| STATS 
    avg_cpu_util = AVG(k8s.container.cpu_request_utilization)* 100,
    max_cpu_util = MAX(k8s.container.cpu_request_utilization)* 100,
    p95_cpu_util = PERCENTILE(k8s.container.cpu_request_utilization, 95) * 100,
    avg_mem_percent = AVG(k8s.container.memory_request_utilization)* 100,
    max_mem_percent = MAX(k8s.container.memory_request_utilization)* 100,
    p95_mem_percent = PERCENTILE(k8s.container.memory_request_utilization, 95) * 100
  BY kubernetes.namespace, kubernetes.container.name
| EVAL 
    cpu_recommendation = CASE(
      avg_cpu_util < 30, "Underutilized - Consider reducing CPU requests/limits",
      avg_cpu_util > 80, "Overutilized - Consider increasing CPU requests/limits",
      "Appropriately sized"
    ),
    mem_recommendation = CASE(
      avg_mem_percent < 30, "Underutilized - Consider reducing memory requests/limits",
      avg_mem_percent > 80, "Overutilized - Consider increasing memory requests/limits",
      "Appropriately sized"
    ),
    throttling_risk = CASE(
      max_cpu_util > 90, "High risk of CPU throttling",
      p95_cpu_util > 80, "Medium risk of CPU throttling",
      "Low risk of CPU throttling"
    ),
    oom_risk = CASE(
      max_mem_percent > 90, "High risk of OOM kills",
      p95_mem_percent > 80, "Medium risk of OOM kills",
      "Low risk of OOM kills"
    )
| SORT avg_cpu_util DESC
| LIMIT 100



------------ TESTS ------------

FROM metrics-*
| WHERE k8s.pod.name IS NOT NULL 
  AND (k8s.container.cpu_request IS NOT NULL OR k8s.container.memory_request IS NOT NULL) AND k8s.namespace.name == "chaos-demo"
| STATS 
    // CPU metrics
    avg_cpu_usage = AVG(k8s.container.cpu.usage),
    avg_cpu_request = AVG(k8s.container.cpu_request),
    avg_cpu_limit = AVG(k8s.container.cpu_limit),
    avg_cpu_request_utilization = AVG(k8s.container.cpu_request_utilization),
    avg_cpu_limit_utilization = AVG(k8s.container.cpu_limit_utilization),
    // Memory metrics
    avg_memory_working_set = AVG(k8s.container.memory.working_set),
    avg_memory_request = AVG(k8s.container.memory_request) / (1024*1024) ,
    avg_memory_limit = AVG(k8s.container.memory_limit) / (1024*1024),
    avg_memory_request_utilization = AVG(k8s.container.memory_request_utilization),
    avg_memory_limit_utilization = AVG(k8s.container.memory_limit_utilization)
  BY k8s.namespace.name, k8s.pod.name, k8s.container.name
| EVAL 
    // CPU allocation status
    cpu_usage_to_request_ratio = CASE(avg_cpu_request > 0, avg_cpu_usage / avg_cpu_request, null),
    cpu_allocation_status = CASE(
      avg_cpu_request_utilization > 0.8, "UNDERALLOCATED",
      avg_cpu_request_utilization < 0.3, "OVERALLOCATED",
      "PROPERLY ALLOCATED"
    ),
    // Memory allocation status
    memory_usage_to_request_ratio = CASE(avg_memory_request > 0, avg_memory_working_set / avg_memory_request, null),
    memory_allocation_status = CASE(
      avg_memory_request_utilization > 0.8, "UNDERALLOCATED",
      avg_memory_request_utilization < 0.3, "OVERALLOCATED",
      "PROPERLY ALLOCATED"
    )
| SORT avg_cpu_request_utilization DESC
| LIMIT 100












TS metrics-*
| WHERE @timestamp >= NOW() - 7 days
| WHERE kubernetes.namespace != null AND kubernetes.container.name != null
| STATS 
    avg_cpu_util = AVG(container.cpu.usage) * 100,
    max_cpu_util = MAX(kcontainer.cpu.usage) * 100,
    p95_cpu_util = PERCENTILE(container.cpu.usage, 95) * 100,
    avg_mem_percent = AVG(k8s.container.memory_limit_utilization) * 100,
    max_mem_percent = MAX(k8s.container.memory_limit_utilization) * 100,
    p95_mem_percent = PERCENTILE(k8s.container.memory_limit_utilization, 95) * 100,
    avg_mem_usage_mb = AVG(k8s.container.memory.usage) / (1024*1024),
    max_mem_usage_mb = MAX(k8s.container.memory.usage) / (1024*1024)
  BY kubernetes.namespace, kubernetes.container.name
| EVAL 
    cpu_recommendation = CASE(
      avg_cpu_util < 30, "Underutilized - Consider reducing CPU requests/limits",
      avg_cpu_util > 80, "Overutilized - Consider increasing CPU requests/limits",
      "Appropriately sized"
    ),
    mem_recommendation = CASE(
      avg_mem_percent < 30, "Underutilized - Consider reducing memory requests/limits",
      avg_mem_percent > 80, "Overutilized - Consider increasing memory requests/limits",
      "Appropriately sized"
    ),
    throttling_risk = CASE(
      max_cpu_util > 90, "High risk of CPU throttling",
      p95_cpu_util > 80, "Medium risk of CPU throttling",
      "Low risk of CPU throttling"
    ),
    oom_risk = CASE(
      max_mem_percent > 90, "High risk of OOM kills",
      p95_mem_percent > 80, "Medium risk of OOM kills",
      "Low risk of OOM kills"
    )
| SORT avg_cpu_util DESC
| LIMIT 100


For ressources utilisation:

Based on the available data, I've created an ES|QL query that calculates CPU usage for pods and compares it to resource allocations and limits. Let me provide you with the complete query and explain how it works:

```sql
FROM metrics-*
| WHERE k8s.pod.name IS NOT NULL AND k8s.container.cpu_request IS NOT NULL
| STATS 
    avg_cpu_usage = AVG(k8s.container.cpu.usage),
    avg_cpu_request = AVG(k8s.container.cpu_request),
    avg_cpu_limit = AVG(k8s.container.cpu_limit),
    avg_cpu_request_utilization = AVG(k8s.container.cpu_request_utilization),
    avg_cpu_limit_utilization = AVG(k8s.container.cpu_limit_utilization)
  BY k8s.namespace.name, k8s.pod.name, k8s.container.name
| EVAL 
    usage_to_request_ratio = avg_cpu_usage / avg_cpu_request,
    allocation_status = CASE(
      avg_cpu_request_utilization > 0.8, "UNDERALLOCATED",
      avg_cpu_request_utilization < 0.3, "OVERALLOCATED",
      "PROPERLY ALLOCATED"
    )
| SORT avg_cpu_request_utilization DESC
| LIMIT 100
```

### How This Query Works:

1. **Data Source**: Searches across all metrics indices (`metrics-*`) that contain Kubernetes pod data
2. **Filtering**: Only includes records that have both pod name and CPU request values
3. **Aggregation**: Calculates average values for:
   - CPU usage
   - CPU request (allocated resources)
   - CPU limit (maximum allowed resources)
   - CPU request utilization (usage/request ratio)
   - CPU limit utilization (usage/limit ratio)
4. **Grouping**: Groups results by namespace, pod name, and container name
5. **Evaluation**: 
   - Calculates the ratio of actual usage to requested resources
   - Determines allocation status based on request utilization:
     - **UNDERALLOCATED**: When utilization is over 80% (container might need more resources)
     - **OVERALLOCATED**: When utilization is under 30% (container has more resources than needed)
     - **PROPERLY ALLOCATED**: When utilization is between 30% and 80%
6. **Sorting**: Orders results by CPU request utilization (highest to lowest)
7. **Limiting**: Shows only the top 100 results

### Results:

The query results show that most containers in your environment are currently "PROPERLY ALLOCATED" based on the defined thresholds. This means their CPU utilization is generally between 30% and 80% of the requested resources.

### Customization Options:

You can adjust this query in several ways:
- Change the thresholds in the CASE statement (currently 0.8 and 0.3) to match your specific requirements
- Add additional metrics like memory usage/allocation
- Filter for specific namespaces or pods
- Change the sorting to focus on different metrics

Let me explain the difference between CPU request utilization and CPU usage in Kubernetes:

### CPU Usage

**CPU usage** is the actual amount of CPU resources being consumed by a container or pod at a given time. It's typically measured in:
- CPU cores (like 0.1 for 100 millicores)
- Or as a raw value representing the absolute CPU consumption

This is the raw measurement of how much CPU the workload is actually using, regardless of what was requested or limited.

### CPU Request Utilization

**CPU request utilization** is a derived metric that represents the ratio between the actual CPU usage and the CPU resources that were requested for that container/pod. It's calculated as:

```
CPU request utilization = CPU usage / CPU request
```

For example:
- If a container requested 0.5 CPU cores and is using 0.4 cores, its request utilization is 0.4/0.5 = 0.8 or 80%
- If a container requested 0.2 CPU cores and is using 0.05 cores, its request utilization is 0.05/0.2 = 0.25 or 25%

### Key Differences

1. **Units and Meaning**:
   - CPU usage is an absolute value (cores or millicores)
   - CPU request utilization is a ratio or percentage (how much of the requested resources are being used)

2. **Purpose**:
   - CPU usage tells you the raw consumption
   - CPU request utilization helps you understand resource efficiency and allocation appropriateness

3. **Resource Planning**:
   - High request utilization (>80%) might indicate the container needs more resources
   - Low request utilization (<30%) might indicate over-provisioning

4. **In Kubernetes Context**:
   - CPU requests are used by the Kubernetes scheduler to place pods on nodes
   - CPU usage is what's actually consumed at runtime
   - The utilization helps you optimize your resource requests

In the query we created earlier, we used CPU request utilization to determine if containers were properly allocated, underallocated, or overallocated based on how efficiently they were using their requested resources.

## Info about differences on kubelestats and k8scluster metrics

- Use kubeletstatsreceiver to alert when a pod is running out of memory (OOM) or a node is CPU throttled.
- Use k8s_cluster to alert when a Deployment has 0/3 replicas ready or a Node status turns to NotReady.

Comparison

Feature	kubeletstatsreceiver	k8s_cluster receiver
Source	Connects to Kubelet API on every node (DaemonSet).	Connects to K8s API Server (Deployment).
Primary Goal	Performance Monitoring (Usage, IO, Saturation).	State Monitoring (Status, Phase, Inventory).
CPU/Mem Metrics	Actual Usage (e.g., 250m used).	Allocated (e.g., 500m requested).
State Metrics	Limited (e.g., simplistic container status).	Rich (e.g., CrashLoopBackOff, ImagePullBackOff reason).
Overhead	Higher (scrapes every node frequently).	Lower (single watch on API server).

## So think about how to compare the requests / forecasted vs actual usage from kubeletstatsreceiver




TS metrics-* | WHERE k8s.namespace.name == "chaos-demo" | STATS AVG(k8s.container.cpu_request_utilization), AVG(k8s.container.cpu_request), AVG(k8s.pod.memory.usage), AVG (k8s.container.memory_request_utilization), AVG(metrics.k8s.container.memory_request_utilization), AVG(metrics.k8s.container.cpu_request_utilization) BY k8s.pod.name

FROM metrics-*
| WHERE k8s.pod.name IS NOT NULL 
  AND k8s.container.cpu_request IS NOT NULL AND k8s.namespace.name == "chaos-demo"
| STATS 
    avg_cpu_usage = AVG(k8s.container.cpu.usage),
    avg_cpu_request = AVG(k8s.container.cpu_request),
    avg_cpu_request_utilization = AVG(metrics.k8s.container.cpu_request_utilization)
  BY k8s.namespace.name, k8s.pod.name, k8s.container.name
| SORT avg_cpu_request_utilization DESC NULLS LAST
| LIMIT 100